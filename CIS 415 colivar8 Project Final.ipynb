{"cells": [{"cell_type": "markdown", "id": "30b0270a-a6b2-4f97-8786-860ad4bcf15e", "metadata": {}, "source": "# Small Data"}, {"cell_type": "code", "execution_count": 6, "id": "2a57dc85-f8fc-4009-8be0-f7c0430beba0", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "PySpark already pre-installed!\n"}], "source": "try:\n  from pyspark.sql import SparkSession\n  pyspark_available = 'Y'\nexcept:\n  pyspark_available = 'N'\n\n# If PySpark is not installed, then go through all these steps\n\nif pyspark_available == 'N':\n  # Update Installer\n  !apt-get update\n\n  # Intsall Java\n  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n\n  # install spark (change the version number if needed)\n  !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n\n  # unzip the spark file to the current folder\n  !tar xf spark-3.0.0-bin-hadoop3.2.tgz\n\n  # set your spark folder to your system path environment.\n  import os\n  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n\n  # install findspark using pip\n  !pip install -q findspark\n\n  import findspark\n  findspark.init()\n\n  from pyspark.sql import SparkSession\n  spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n  # To access Google Cloud Storage\n  from google.cloud import storage\n  import google.auth\n\n  !pip install gcsfs\n  import gcsfs\n\n  from google.colab import auth\n  auth.authenticate_user()\n\n  credentials, default_project_id = google.auth.default()\n  !gcloud config set project {project_id}\nelse:\n    # Spark / PySpark already pre-installed in the environment\n    print(\"PySpark already pre-installed!\")"}, {"cell_type": "code", "execution_count": 8, "id": "d1c7224a-95bd-4fd9-b3a3-e426f2a7020f", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Package imports done\n"}], "source": "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, HashingTF\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.sql.functions import lit\n\n# Normally, we will not need Pandas if we are working with PySpark (since PySpark provides the dataframe capabilities)\n# However, we will need pandas just for one step in this task: for reading files from Google Cloud Storage\n#     This is because it's not straightforward to set the configurations correctly for letting spark read from GCS\nfrom pandas import DataFrame, read_csv\n# Added this line in SP24 since pandas removed iteritems from DataFrame object in 2024\nDataFrame.iteritems = DataFrame.items\n\nfrom google.cloud import storage\n\nclient = storage.Client()\n\nprint(f\"Package imports done\")"}, {"cell_type": "code", "execution_count": 9, "id": "dd862873-2e4e-44e8-acdc-7761f85bd6f9", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ProjectID (and not the Project Name) is: colivar8-cis415-fall24a\nBucket name is: colivar8_data_for_gcp_labs\nWe will run this task for SMALL DATA (small_fraud_detection_dataset.csv)\nFull path to the data file is gs://colivar8_data_for_gcp_labs/data_for_fraud_project/small_fraud_detection_dataset.csv\n"}], "source": " #Configure the following to use YOUR GCP setup\n\n# 1. Configure the Project ID (not Project Name!!!) as per your GCP Dataproc setup\nproject_id = 'colivar8-cis415-fall24a'\n\n# 2. Configure Bucket name as per your Google Cloud Storage setup\nbucket = 'colivar8_data_for_gcp_labs'\n\n# 3. Configure the path to the movie reviews data file as per your Google Cloud Storage setup\n#    If your setup is exactly as per the instructions in GCP Lab 1c and in this lab:\n#       --- you will not need to make any changes to the below line.\n#    If your setup is different (due to whatever reason - doesn't matter),\n#       --- just update the below line to reflect the path as per YOUR Google Cloud Storage setup\npath_to_data_files = \"/data_for_fraud_project/\"\n\n# 3. Configure the appropriate data file to be used for the task\n#       Uncomment one of the two lines below based on the following:\n#          In Google Colab, you should build/test with SMALL DATA\n#          In GCP, first you should run with SMALL DATA\n#             and finally, you should run with BIG DATA\n\n#fraud_detection = \"big_fraud_detection_dataset.csv\"\nfraud_detection = \"small_fraud_detection_dataset.csv\"\n\n# Lastly, we will build the full path of the data file and confirm it's correct\n# You do not need to change this line\nfull_file_path = \"gs://\" + bucket + path_to_data_files + fraud_detection\n\n# Let's print out all the configurations and ensure that they are correct\nprint(f\"ProjectID (and not the Project Name) is: {project_id}\")\nprint(f\"Bucket name is: {bucket}\")\nif fraud_detection == \"small_fraud_detection_dataset.csv\":\n  print(f\"We will run this task for SMALL DATA ({fraud_detection})\")\nelif fraud_detection == \"big_fraud_detection_dataset.csv\":\n  print(f\"We will run this task for BIG DATA ({fraud_detection})\")\nelse:\n  print(\"-\"*20)\n  print(f\"Incorrect data file name - {fraud_detection}!! CHECK & FIX!!!\")\n  print(\"-\"*20)\n\nprint(f\"Full path to the data file is {full_file_path}\")"}, {"cell_type": "code", "execution_count": 10, "id": "29c8774b-5342-4d29-ac90-6fc397c81369", "metadata": {"tags": []}, "outputs": [], "source": "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()"}, {"cell_type": "code", "execution_count": 11, "id": "16021071-8b7f-421c-a2da-30213bdd7c51", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, isnan\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.sql.functions import split, col"}, {"cell_type": "code", "execution_count": 12, "id": "e5d1bf4a-c465-4332-97a2-e2d034db82cc", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Reading the data file: gs://colivar8_data_for_gcp_labs/data_for_fraud_project/small_fraud_detection_dataset.csv\nPandas data frame is ready with the data.  Now converting it to a Spark Dataframe\nSpark data frame is ready with the data.  Let's check the first few rows...\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:17:11 WARN TaskSetManager: Stage 0 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|Transaction_Amount|Transaction_Type|Account_Age|Transaction_Location|Transaction_Time|Fraudulent|Customer_ID|Loan_Amount|Loan_Purpose|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|           2199.93|        In-store|       21.0|             Chicago|            0:57|         0|  CUST91186|   36021.94|    Business|\n|            875.52|        In-store|       18.0|         Los Angeles|           11:41|         0|  CUST15505|   10584.69|    Personal|\n|           1109.42|        In-store|       16.0|       San Francisco|           17:26|         0|  CUST48110|   91714.73|   Education|\n|           4052.24|        In-store|        8.0|             Houston|           23:43|         0|  CUST15744|   36170.59|   Education|\n|            571.92|          Online|       14.0|         Los Angeles|           18:21|         0|  CUST29211|   26597.44|    Business|\n|           3343.15|          Online|       29.0|            New York|           21:57|         0|  CUST55289|   91207.61|    Business|\n|            770.33|          Online|       21.0|             Houston|           16:48|         0|  CUST36185|   75468.03|    Personal|\n|            225.48|          Online|        5.0|             Houston|            8:56|         0|  CUST59757|   33761.05|    Business|\n|           3464.02|             ATM|       17.0|             Chicago|            6:53|         0|  CUST96848|   16119.27|    Business|\n|            540.58|        In-store|        1.0|       San Francisco|           21:37|         0|  CUST52057|   78142.46|    Mortgage|\n|           4376.04|             ATM|       22.0|             Chicago|           23:35|         0|  CUST61312|   83975.29|    Business|\n|            4208.0|          Online|       10.0|             Chicago|            8:40|         0|  CUST68373|   89823.95|   Education|\n|            174.74|          Online|       10.0|             Chicago|           22:25|         0|  CUST89736|   41821.86|    Business|\n|           1593.82|          Online|        1.0|       San Francisco|           15:45|         0|  CUST26467|   84762.64|    Personal|\n|            357.67|          Online|       15.0|            New York|            2:13|         0|  CUST38381|   62939.97|    Mortgage|\n|              15.1|          Online|       18.0|       San Francisco|           22:31|         0|  CUST49880|   45285.66|   Education|\n|           3521.19|        In-store|        9.0|             Chicago|            13:9|         0|  CUST65356|   62756.45|    Business|\n|           4302.17|        In-store|       16.0|         Los Angeles|           20:46|         0|  CUST26824|   24567.23|    Mortgage|\n|           1765.15|          Online|       29.0|       San Francisco|             3:2|         0|  CUST81623|   38194.88|    Personal|\n|            942.09|             ATM|       10.0|             Chicago|            3:28|         0|  CUST18432|   58680.66|   Education|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:17:18 WARN TaskSetManager: Stage 1 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n[Stage 1:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Total number of records from data file = 49961\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read the data file\nprint(f\"Reading the data file: {full_file_path}\")\n\npandas_df = read_csv(full_file_path, sep=\",\")\nprint(f\"Pandas data frame is ready with the data.  Now converting it to a Spark Dataframe\")\ncleaned_pandas_df = pandas_df.dropna()\n\nspark_df = spark.createDataFrame(cleaned_pandas_df)\nprint(f\"Spark data frame is ready with the data.  Let's check the first few rows...\")\n\n# Check the first few records in the data\nspark_df.show()\n\n# How many records got loaded?\nprint(f\"Total number of records from data file = {spark_df.count()}\")"}, {"cell_type": "code", "execution_count": 13, "id": "b4007b70-daed-4ad9-8a4a-a6bd3e312894", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Transaction_Amount: double (nullable = true)\n |-- Transaction_Type: string (nullable = true)\n |-- Account_Age: double (nullable = true)\n |-- Transaction_Location: string (nullable = true)\n |-- Transaction_Time: string (nullable = true)\n |-- Fraudulent: long (nullable = true)\n |-- Customer_ID: string (nullable = true)\n |-- Loan_Amount: double (nullable = true)\n |-- Loan_Purpose: string (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:17:45 WARN TaskSetManager: Stage 4 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|Transaction_Amount|Transaction_Type|Account_Age|Transaction_Location|Transaction_Time|Fraudulent|Customer_ID|Loan_Amount|Loan_Purpose|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|           2199.93|        In-store|       21.0|             Chicago|            0:57|         0|  CUST91186|   36021.94|    Business|\n|            875.52|        In-store|       18.0|         Los Angeles|           11:41|         0|  CUST15505|   10584.69|    Personal|\n|           1109.42|        In-store|       16.0|       San Francisco|           17:26|         0|  CUST48110|   91714.73|   Education|\n|           4052.24|        In-store|        8.0|             Houston|           23:43|         0|  CUST15744|   36170.59|   Education|\n|            571.92|          Online|       14.0|         Los Angeles|           18:21|         0|  CUST29211|   26597.44|    Business|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:17:46 WARN TaskSetManager: Stage 5 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:17:53 WARN TaskSetManager: Stage 8 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:09 WARN TaskSetManager: Stage 9 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:11 WARN TaskSetManager: Stage 10 contains a task of very large size (1412 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:12 WARN TaskSetManager: Stage 11 contains a task of very large size (1412 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:12 WARN TaskSetManager: Stage 12 contains a task of very large size (1412 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:12 WARN TaskSetManager: Stage 13 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:15 WARN TaskSetManager: Stage 14 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:17 WARN TaskSetManager: Stage 17 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:18 WARN TaskSetManager: Stage 18 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:19 WARN TaskSetManager: Stage 19 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:21 WARN TaskSetManager: Stage 21 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:24 WARN TaskSetManager: Stage 23 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:26 WARN TaskSetManager: Stage 25 contains a task of very large size (1411 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:28 WARN TaskSetManager: Stage 27 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:30 WARN TaskSetManager: Stage 29 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:18:33 WARN TaskSetManager: Stage 31 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Logistic Regression AUC: 0.508714282327229\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:18:35 WARN TaskSetManager: Stage 42 contains a task of very large size (1367 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Random Forest AUC: 0.4967845023863872\n"}], "source": "# Code that GPT gave me for step 2 in the project\n# Step 2: Load the small dataset\ndata = spark_df\n\n# Step 3: EDA - Display dataset schema and first few rows\ndata.printSchema()\ndata.show(5)\n\n# Step 5: Feature engineering\n# Convert categorical variables into numeric format using StringIndexer\nindexer = StringIndexer(inputCols=[\"Transaction_Type\", \"Transaction_Location\"],\n                        outputCols=[\"Transaction_Type_Index\", \"Transaction_Location_Index\"])\ndata = indexer.fit(data).transform(data)\n\n# Step 6: Feature selection\n# Select relevant features and target variable for modeling\nassembler = VectorAssembler(\n    inputCols=[\"Transaction_Amount\", \"Account_Age\", \"Transaction_Type_Index\", \"Transaction_Location_Index\"],\n    outputCol=\"features\"\n)\ndata = assembler.transform(data).select(\"features\", \"Fraudulent\")\n\n# Step 7: Splitting data into training and validation sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n\n# Step 8: Training the first model - Logistic Regression\nlr = LogisticRegression(labelCol=\"Fraudulent\", featuresCol=\"features\")\nlr_model = lr.fit(train_data)\n\n# Step 9: Training the second model - Random Forest\nrf = RandomForestClassifier(labelCol=\"Fraudulent\", featuresCol=\"features\", numTrees=100)\nrf_model = rf.fit(train_data)\n\n# Step 10: Evaluating both models\nevaluator = BinaryClassificationEvaluator(labelCol=\"Fraudulent\", rawPredictionCol=\"rawPrediction\")\n\n# Logistic Regression evaluation\nlr_predictions = lr_model.transform(test_data)\nlr_auc = evaluator.evaluate(lr_predictions)\nprint(f\"Logistic Regression AUC: {lr_auc}\")\n\n# Random Forest evaluation\nrf_predictions = rf_model.transform(test_data)\nrf_auc = evaluator.evaluate(rf_predictions)\nprint(f\"Random Forest AUC: {rf_auc}\")\n\n# Step 11: Conclusion - Once the ML pipeline is tested in the small data environment,\n# it can be deployed in the big data environment, starting with the small dataset\n# and scaling up to the larger dataset.\n\nspark.stop()"}, {"cell_type": "markdown", "id": "cd49e317-53aa-4591-b399-7f6461ee1e07", "metadata": {}, "source": "# Big Data"}, {"cell_type": "code", "execution_count": 17, "id": "139db1b9-3351-4ff3-be68-19a2d7d17c9e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Package imports done\n"}], "source": "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, HashingTF\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.sql.functions import lit\n\n# Normally, we will not need Pandas if we are working with PySpark (since PySpark provides the dataframe capabilities)\n# However, we will need pandas just for one step in this task: for reading files from Google Cloud Storage\n#     This is because it's not straightforward to set the configurations correctly for letting spark read from GCS\nfrom pandas import DataFrame, read_csv\n# Added this line in SP24 since pandas removed iteritems from DataFrame object in 2024\nDataFrame.iteritems = DataFrame.items\n\nfrom google.cloud import storage\n\nclient = storage.Client()\n\nprint(f\"Package imports done\")"}, {"cell_type": "code", "execution_count": 18, "id": "36301f12-8e38-4b04-981c-693e334336fe", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ProjectID (and not the Project Name) is: colivar8-cis415-fall24a\nBucket name is: colivar8_data_for_gcp_labs\nWe will run this task for BIG DATA (big_fraud_detection_dataset.csv)\nFull path to the data file is gs://colivar8_data_for_gcp_labs/data_for_fraud_project/big_fraud_detection_dataset.csv\n"}], "source": " #Configure the following to use YOUR GCP setup\n\n# 1. Configure the Project ID (not Project Name!!!) as per your GCP Dataproc setup\nproject_id = 'colivar8-cis415-fall24a'\n\n# 2. Configure Bucket name as per your Google Cloud Storage setup\nbucket = 'colivar8_data_for_gcp_labs'\n\n# 3. Configure the path to the movie reviews data file as per your Google Cloud Storage setup\n#    If your setup is exactly as per the instructions in GCP Lab 1c and in this lab:\n#       --- you will not need to make any changes to the below line.\n#    If your setup is different (due to whatever reason - doesn't matter),\n#       --- just update the below line to reflect the path as per YOUR Google Cloud Storage setup\npath_to_data_files = \"/data_for_fraud_project/\"\n\n# 3. Configure the appropriate data file to be used for the task\n#       Uncomment one of the two lines below based on the following:\n#          In Google Colab, you should build/test with SMALL DATA\n#          In GCP, first you should run with SMALL DATA\n#             and finally, you should run with BIG DATA\n\nfraud_detection = \"big_fraud_detection_dataset.csv\"\n#fraud_detection = \"small_fraud_detection_dataset.csv\"\n\n# Lastly, we will build the full path of the data file and confirm it's correct\n# You do not need to change this line\nfull_file_path = \"gs://\" + bucket + path_to_data_files + fraud_detection\n\n# Let's print out all the configurations and ensure that they are correct\nprint(f\"ProjectID (and not the Project Name) is: {project_id}\")\nprint(f\"Bucket name is: {bucket}\")\nif fraud_detection == \"small_fraud_detection_dataset.csv\":\n  print(f\"We will run this task for SMALL DATA ({fraud_detection})\")\nelif fraud_detection == \"big_fraud_detection_dataset.csv\":\n  print(f\"We will run this task for BIG DATA ({fraud_detection})\")\nelse:\n  print(\"-\"*20)\n  print(f\"Incorrect data file name - {fraud_detection}!! CHECK & FIX!!!\")\n  print(\"-\"*20)\n\nprint(f\"Full path to the data file is {full_file_path}\")"}, {"cell_type": "code", "execution_count": 19, "id": "3f2fd1c3-9530-4b13-998e-abf77571cd24", "metadata": {"tags": []}, "outputs": [], "source": "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()"}, {"cell_type": "code", "execution_count": 20, "id": "a0408577-4b08-4e9f-b36f-c6b166f8640d", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, isnan\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.sql.functions import split, col"}, {"cell_type": "code", "execution_count": 21, "id": "06fc63b4-74b0-4cb7-9096-ef9ee06ce9fa", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Reading the data file: gs://colivar8_data_for_gcp_labs/data_for_fraud_project/big_fraud_detection_dataset.csv\nPandas data frame is ready with the data.  Now converting it to a Spark Dataframe\nSpark data frame is ready with the data.  Let's check the first few rows...\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:24:42 WARN TaskSetManager: Stage 0 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:24:49 WARN TaskSetManager: Stage 1 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|Transaction_Amount|Transaction_Type|Account_Age|Transaction_Location|Transaction_Time|Fraudulent|Customer_ID|Loan_Amount|Loan_Purpose|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|           1645.59|        In-store|       16.0|            New York|           19:12|         0|  CUST70769|   97913.21|    Business|\n|           2852.75|          Online|        5.0|             Houston|            6:57|         0|  CUST79634|   69864.83|   Education|\n|            730.57|          Online|        9.0|            New York|           17:37|         0|  CUST24562|    47731.3|    Personal|\n|           1008.33|        In-store|       18.0|            New York|             0:0|         0|  CUST85889|    4698.02|    Business|\n|           2298.86|             ATM|       24.0|             Chicago|           20:50|         1|  CUST81425|   36428.48|    Mortgage|\n|            857.06|             ATM|       21.0|             Houston|           19:50|         0|  CUST86542|   53683.85|    Personal|\n|            3251.8|        In-store|        4.0|            New York|           11:23|         0|  CUST46141|   41163.27|    Personal|\n|           3347.66|        In-store|        6.0|         Los Angeles|            6:47|         0|  CUST78389|    8522.97|   Education|\n|           2110.94|          Online|       27.0|            New York|           20:49|         0|  CUST32823|   61125.86|    Business|\n|           2746.24|             ATM|       28.0|             Chicago|            9:39|         0|  CUST84259|    29431.9|    Personal|\n|            820.03|          Online|       24.0|            New York|           13:13|         0|  CUST46670|   46446.35|    Personal|\n|           4096.56|             ATM|       17.0|         Los Angeles|           13:52|         0|  CUST61620|   98601.47|   Education|\n|           4349.79|             ATM|       13.0|             Houston|           13:48|         0|  CUST80866|    78143.9|    Personal|\n|           1657.63|        In-store|       14.0|             Chicago|           23:30|         0|  CUST33240|   28697.93|   Education|\n|           4835.68|        In-store|        5.0|             Houston|            22:4|         0|  CUST87461|   71521.21|    Business|\n|           4926.85|        In-store|       24.0|         Los Angeles|            23:6|         0|  CUST64516|    93970.3|    Business|\n|           4080.89|        In-store|       22.0|             Chicago|           13:10|         0|  CUST24871|    74152.3|    Personal|\n|             194.6|        In-store|       12.0|             Houston|           22:52|         0|  CUST60331|   10368.71|    Business|\n|           1912.16|        In-store|       21.0|             Houston|            1:18|         0|  CUST27721|   46592.94|    Personal|\n|           4464.16|             ATM|       17.0|             Chicago|           23:18|         0|  CUST93250|    53914.0|   Education|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 1:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Total number of records from data file = 999001\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read the data file\nprint(f\"Reading the data file: {full_file_path}\")\n\npandas_df = read_csv(full_file_path, sep=\",\")\nprint(f\"Pandas data frame is ready with the data.  Now converting it to a Spark Dataframe\")\ncleaned_pandas_df = pandas_df.dropna()\n\nspark_df = spark.createDataFrame(cleaned_pandas_df)\nprint(f\"Spark data frame is ready with the data.  Let's check the first few rows...\")\n\n# Check the first few records in the data\nspark_df.show()\n\n# How many records got loaded?\nprint(f\"Total number of records from data file = {spark_df.count()}\")"}, {"cell_type": "code", "execution_count": null, "id": "0a8e704a-a29d-4582-a18a-1bfb5fc2a9d0", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Transaction_Amount: double (nullable = true)\n |-- Transaction_Type: string (nullable = true)\n |-- Account_Age: double (nullable = true)\n |-- Transaction_Location: string (nullable = true)\n |-- Transaction_Time: string (nullable = true)\n |-- Fraudulent: long (nullable = true)\n |-- Customer_ID: string (nullable = true)\n |-- Loan_Amount: double (nullable = true)\n |-- Loan_Purpose: string (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:26:40 WARN TaskSetManager: Stage 4 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|Transaction_Amount|Transaction_Type|Account_Age|Transaction_Location|Transaction_Time|Fraudulent|Customer_ID|Loan_Amount|Loan_Purpose|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\n|           1645.59|        In-store|       16.0|            New York|           19:12|         0|  CUST70769|   97913.21|    Business|\n|           2852.75|          Online|        5.0|             Houston|            6:57|         0|  CUST79634|   69864.83|   Education|\n|            730.57|          Online|        9.0|            New York|           17:37|         0|  CUST24562|    47731.3|    Personal|\n|           1008.33|        In-store|       18.0|            New York|             0:0|         0|  CUST85889|    4698.02|    Business|\n|           2298.86|             ATM|       24.0|             Chicago|           20:50|         1|  CUST81425|   36428.48|    Mortgage|\n+------------------+----------------+-----------+--------------------+----------------+----------+-----------+-----------+------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/10 04:26:47 WARN TaskSetManager: Stage 5 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:05 WARN TaskSetManager: Stage 8 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:27 WARN TaskSetManager: Stage 9 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:38 WARN TaskSetManager: Stage 10 contains a task of very large size (27683 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:39 WARN TaskSetManager: Stage 11 contains a task of very large size (27683 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:39 WARN TaskSetManager: Stage 12 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:40 WARN TaskSetManager: Stage 13 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:41 WARN TaskSetManager: Stage 14 contains a task of very large size (27683 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:42 WARN TaskSetManager: Stage 15 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:50 WARN TaskSetManager: Stage 18 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:27:54 WARN TaskSetManager: Stage 19 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:28:02 WARN TaskSetManager: Stage 20 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:28:12 WARN TaskSetManager: Stage 22 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:28:35 WARN TaskSetManager: Stage 24 contains a task of very large size (27683 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:28:47 WARN TaskSetManager: Stage 26 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:29:03 WARN TaskSetManager: Stage 28 contains a task of very large size (27683 KiB). The maximum recommended task size is 1000 KiB.\n24/10/10 04:29:22 WARN TaskSetManager: Stage 30 contains a task of very large size (27676 KiB). The maximum recommended task size is 1000 KiB.\n[Stage 30:>                                                         (0 + 2) / 2]\r"}], "source": "# Code that GPT gave me for step 2 in the project\n# Step 2: Load the small dataset\ndata = spark_df\n\n# Step 3: EDA - Display dataset schema and first few rows\ndata.printSchema()\ndata.show(5)\n\n# Step 5: Feature engineering\n# Convert categorical variables into numeric format using StringIndexer\nindexer = StringIndexer(inputCols=[\"Transaction_Type\", \"Transaction_Location\"],\n                        outputCols=[\"Transaction_Type_Index\", \"Transaction_Location_Index\"])\ndata = indexer.fit(data).transform(data)\n\n# Step 6: Feature selection\n# Select relevant features and target variable for modeling\nassembler = VectorAssembler(\n    inputCols=[\"Transaction_Amount\", \"Account_Age\", \"Transaction_Type_Index\", \"Transaction_Location_Index\"],\n    outputCol=\"features\"\n)\ndata = assembler.transform(data).select(\"features\", \"Fraudulent\")\n\n# Step 7: Splitting data into training and validation sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n\n# Step 8: Training the first model - Logistic Regression\nlr = LogisticRegression(labelCol=\"Fraudulent\", featuresCol=\"features\")\nlr_model = lr.fit(train_data)\n\n# Step 9: Training the second model - Random Forest\nrf = RandomForestClassifier(labelCol=\"Fraudulent\", featuresCol=\"features\", numTrees=100)\nrf_model = rf.fit(train_data)\n\n# Step 10: Evaluating both models\nevaluator = BinaryClassificationEvaluator(labelCol=\"Fraudulent\", rawPredictionCol=\"rawPrediction\")\n\n# Logistic Regression evaluation\nlr_predictions = lr_model.transform(test_data)\nlr_auc = evaluator.evaluate(lr_predictions)\nprint(f\"Logistic Regression AUC: {lr_auc}\")\n\n# Random Forest evaluation\nrf_predictions = rf_model.transform(test_data)\nrf_auc = evaluator.evaluate(rf_predictions)\nprint(f\"Random Forest AUC: {rf_auc}\")\n\n# Step 11: Conclusion - Once the ML pipeline is tested in the small data environment,\n# it can be deployed in the big data environment, starting with the small dataset\n# and scaling up to the larger dataset.\n\nspark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "12cc673d-8d25-4d53-a510-607f00dc0de2", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}